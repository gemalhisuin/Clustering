{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "################# Gemal Hisuin #############################################################\n",
    "############# Fundamentals of Machine Learning - TP3 #######################################\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FORWARD ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   t1 ------- t2 -------- t3\n",
      "[[0.16666667 0.02777778 0.        ]\n",
      " [0.         0.02777778 0.        ]\n",
      " [0.         0.         0.00925926]]\n"
     ]
    }
   ],
   "source": [
    "###### computation of the probability of an observation sequence with the forward algorithm ######\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "'''\n",
    "    The computed probability on the current time step would be used to derive the probability of the next time \n",
    "    step in this forward sequence algorithm. We will calculate the probability that the Hidden Markov Model \n",
    "    will be in a certain hidden state (s) at a specific time stage (t) given a series of visible states (VT).\n",
    "'''\n",
    "'''\n",
    "Input Arguments:\n",
    "        transition_probability: State transition probability matrix of dimension 3 x 3\n",
    "        start_probability: Initial state distribution  of dimension 3\n",
    "        emission_probability: Emission probability of dimension 3*3\n",
    "        visible_State: Observation sequence of length 3\n",
    "\n",
    "Returns:\n",
    "        forward_sequence_prob: Output probability matrix of dimension 3 x 3\n",
    "\n",
    "\n",
    "'''\n",
    "hidden_states={'Box-1', 'Box-2', 'Box-3'}\n",
    "## Assign observation states to numeric numbers {a=0; b=1; e=2} for simplicicty\n",
    "visible_state = np.array((0,1,2)) # 0 represents {a}; 1 represents {b}; 2 represents {e} \n",
    "\n",
    "##### set the values for transition probability, emission probabilities and initial distribution.\n",
    "# initial distribution    \n",
    "start_probability = np.array((1/3, \n",
    "                              1/3, \n",
    "                              1/3))\n",
    " \n",
    "# Transition Probabilities    \n",
    "transition_probability = np.array(((1/3, 1/3, 1/3), \n",
    "                                   (1/3, 1/3, 1/3), \n",
    "                                   (1/3, 1/3, 1/3)))\n",
    "# Emission Probabilities \n",
    "emission_probability = np.array(((0.5, 0.5, 0), \n",
    "                                 (0, 0.5, 0),\n",
    "                                 (0, 0, 0.5)))\n",
    "\n",
    "\n",
    "'''t will start from 0 and ends at T-1.\n",
    "we create the forward_sequence_prob matrix with 3 Columns and T Rows.\n",
    "We multiply start_probability with emission probabaility to calculate α0(0),α1(0)\n",
    "Starting from index 0 we will loop through the time steps.\n",
    "We will use the same method to calculate t until T-1.\n",
    "Print all of the forward_sequence_prob values.\n",
    "'''\n",
    "def forward(visible_state, transition_probability, emission_probability, start_probability):\n",
    "    forward_sequence_prob = np.zeros((visible_state.shape[0], transition_probability.shape[0]))\n",
    "    forward_sequence_prob[0, :] = start_probability * emission_probability[:, visible_state[0]]\n",
    " \n",
    "    for t in range(1, visible_state.shape[0]):\n",
    "        for j in range(transition_probability.shape[0]):\n",
    "            forward_sequence_prob[t, j] = forward_sequence_prob[t - 1].dot(transition_probability[:, j]) * emission_probability[j, visible_state[t]]\n",
    "    return forward_sequence_prob\n",
    " \n",
    "forward_sequence_prob = forward(visible_state, transition_probability, emission_probability, start_probability)\n",
    "forward_sequence_prob = np.transpose(forward_sequence_prob)\n",
    "print(\"   t1 ------- t2 -------- t3\")\n",
    "\n",
    "'''\n",
    "The result shows the probability of a given {a,b,e} sequence.\n",
    "    t1 ------- t2 -------- t3\n",
    "[[0.16666667 0.02777778 0.        ]\n",
    " [0.         0.02777778 0.        ]\n",
    " [0.         0.         0.00925926]]\n",
    "The forward sequance algorithm yields the same results for the given HMM observation as the results I obtained \n",
    "in the Modeling section(1) of this report. The value obtained for {a,b,e} at stage t3 on 'Modeling section' is 0.009259,\n",
    "which is the same with the value obtained on this python program\n",
    " \n",
    "'''\n",
    "print(forward_sequence_prob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BACKWARD ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   t1 ----- t2 --- t3\n",
      "[[0.05555556 0.16666667 1.        ]\n",
      " [0.05555556 0.16666667 1.        ]\n",
      " [0.05555556 0.16666667 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "###### computation of the probability of an observation sequence with the backward algorithm ######\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "'''\n",
    "Input Arguments:\n",
    "        transition_probability: State transition probability matrix of dimension 3 x 3\n",
    "        emission_probability: Emission probability of dimension 3*3\n",
    "        visible_State: Observation sequence of length 3\n",
    "\n",
    "Returns:\n",
    "        backward_sequence_prob: Output probability matrix of dimension 3 x 3\n",
    "\n",
    "\n",
    "'''\n",
    "'''\n",
    " We usually don't need to know the Backward Algorithm to solve the Likelihood problem. Its description and solution is \n",
    " a strong smoothness measure to demonstrate that the forward algorithm functions appropriately. \n",
    " The backward algorithm is equivalent to the forward algorithm, but it goes backward in time as the name implies. \n",
    "'''\n",
    "hidden_states={'Box-1', 'Box-2', 'Box-3'}\n",
    "## Assign observation states to numeric numbers {a=0; b=1; e=2} for simplicicty\n",
    "visible_state = np.array((0,1,2)) # 0 represents {a}; 1 represents {b}; 2 represents {e} \n",
    " \n",
    "# Transition Probabilities    \n",
    "transition_probability = np.array(((1/3, 1/3, 1/3), \n",
    "                                   (1/3, 1/3, 1/3), \n",
    "                                   (1/3, 1/3, 1/3)))\n",
    "# Emission Probabilities \n",
    "emission_probability = np.array(((0.5, 0.5, 0), \n",
    "                                 (0, 0.5, 0),\n",
    "                                 (0, 0, 0.5)))\n",
    "\n",
    " \n",
    "'''\n",
    "The backward variables of any state at the end of the observation series are equal to 1.\n",
    "We would be able to measure the backward variable of the previous state by going backwards. \n",
    "\n",
    "''' \n",
    "def backward(visible_state, transition_probability, emission_probability):\n",
    "    backward_sequence_prob = np.zeros((visible_state.shape[0], transition_probability.shape[0]))\n",
    " \n",
    "    # set backward_sequence_prob(T) = 1\n",
    "    backward_sequence_prob[visible_state.shape[0] - 1] = np.ones((transition_probability.shape[0]))\n",
    " \n",
    "    # From T-1, make a backward loop.\n",
    "    for t in range(visible_state.shape[0] - 2, -1, -1): # The main loop would be T-2 to 0 because of python indexing.\n",
    "        for j in range(transition_probability.shape[0]):\n",
    "            backward_sequence_prob[t, j] = (backward_sequence_prob[t + 1] * emission_probability[:, visible_state[t + 1]]).dot(transition_probability[j, :])\n",
    " \n",
    "    return backward_sequence_prob\n",
    " \n",
    " \n",
    "backward_sequence_prob = backward(visible_state, transition_probability, emission_probability)\n",
    "backward_sequence_prob = np.transpose(backward_sequence_prob)\n",
    "print(\"   t1 ----- t2 --- t3\")\n",
    "\n",
    "'''\n",
    "This is accomplished by adding up three multiplication values:\n",
    "\n",
    "For instance,\n",
    "Transition probability from Box-1 to Box-1 (1/3) times the emission probability of {a} originating from \n",
    "Box-1 state at time t+1 (0.5) times the backward variable of the Sunny State at time t+1 (1) \n",
    "1/3*0.5*1 = 0.16667\n",
    "\n",
    "Transition probability from Box-1 to Box-2 (1/3) times the emission probability of {a} originating from \n",
    "Box-2 state at time t+1 (0.0) times the backward variable of the Sunny State at time t+1 (1) \n",
    "1/3*0*1 = 0\n",
    "\n",
    "Transition probability from Box-1 to Box-1 (1/3) times the emission probability of {a} originating from \n",
    "Box-3 state at time t+1 (0.0) times the backward variable of the Sunny State at time t+1 (1)\n",
    "1/3*0*1 = 0\n",
    "\n",
    "We will sum the three multiplication results \n",
    "0.16667+0+0 = 0.166667\n",
    "0.16667  will be the probability of Box-1 at time step-2 (t2)\n",
    "'''\n",
    "print(backward_sequence_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VITERBI ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely sequence of state is: ['Box-1', 'Box-1', 'Box-3']\n"
     ]
    }
   ],
   "source": [
    "#### The extraction of the most likely sequence of states explaining a sequence of observation with the Viterbi Algorithm.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Input Arguments:\n",
    "        transition_probability: State transition probability matrix of dimension 3 x 3\n",
    "        start_probability: Initial state distribution  of dimension 3\n",
    "        emission_probability: Emission probability of dimension 3*3\n",
    "        visible_State: Observation sequence of length 3\n",
    "\n",
    "Returns:\n",
    "        Optimal state sequence of length 3\n",
    "\n",
    "'''\n",
    "hidden_states={'Box-1', 'Box-2', 'Box-3'}\n",
    "## Assign observation states to numeric numbers {a=0; b=1; e=2} for simplicicty\n",
    "visible_state = np.array((0,1,2)) # 0 represents {a}; 1 represents {b}; 2 represents {e} \n",
    "\n",
    "\n",
    "observations = ('a', 'b', 'e')\n",
    "\n",
    "# Equal Probabilities for the initial distribution    \n",
    "start_probability = np.array((1/3, \n",
    "                              1/3, \n",
    "                              1/3))\n",
    " \n",
    "# Transition Probabilities    \n",
    "transition_probability = np.array(((1/3, 1/3, 1/3), \n",
    "                                   (1/3, 1/3, 1/3), \n",
    "                                   (1/3, 1/3, 1/3)))\n",
    "# Emission Probabilities \n",
    "emission_probability = np.array(((0.5, 0.5, 0), \n",
    "                                 (0, 0.5, 0),\n",
    "                                 (0, 0, 0.5)))\n",
    "\n",
    "'''\n",
    "Using viterbi algorithm,\n",
    "We will determine what the most probable next hidden state will be at each time step (t) and each hidden state.\n",
    "The aim is to represent the highest probability along a single path that ends at state t.\n",
    "\n",
    "Steps:\n",
    "- To define the state that maximizes at each time step t, we must first find the sequence of hidden states for all observations.\n",
    "- We'll use backpointer to backtrack the most likely hidden state after finding the last hidden state using maximum likelihood.\n",
    "\n",
    "'''\n",
    "def viterbi(visible_state, transition_probability, emission_probability, start_probability):\n",
    "    VS = visible_state.shape[0]\n",
    "    TP = transition_probability.shape[0]\n",
    " \n",
    "    last_s = np.zeros((VS, TP))\n",
    "    last_s[0, :] = start_probability * emission_probability[:, visible_state[0]]\n",
    " \n",
    "    previous_s = np.zeros((VS - 1, TP))\n",
    " \n",
    "    for t in range(1, VS):\n",
    "        for j in range(TP):\n",
    "            probability = last_s[t - 1] + transition_probability[:, j] + emission_probability[j, visible_state[t]]\n",
    "            previous_s[t - 1, j] = np.argmax(probability) # Provided the previous state at time t, this is our most probable state (1)\n",
    "            last_s[t, j] = np.max(probability) # The likelihood of the most probable state of state (2)\n",
    "   \n",
    "    arr_list = np.zeros(VS) # Path Array\n",
    "    last_state = np.argmax(last_s[VS - 1, :]) # Find the hidden state that is most probable to be the last one \n",
    " \n",
    "    arr_list[0] = last_state\n",
    " \n",
    "    backtrack_index = 1\n",
    "    for i in range(VS - 2, -1, -1):\n",
    "        arr_list[backtrack_index] = previous_s[i, int(last_state)]\n",
    "        last_state = previous_s[i, int(last_state)]\n",
    "        backtrack_index += 1\n",
    "        \n",
    "    arr_list = np.flip(arr_list, axis=0)# As we were going backwards, flip the route array.\n",
    " \n",
    "    # Convert numerical values into hidden states.\n",
    "    result = []\n",
    "    for bp in arr_list:\n",
    "        if bp == 0:\n",
    "            result.append(\"Box-1\")\n",
    "        elif bp == 1:\n",
    "            result.append(\"Box-2\")\n",
    "        else:\n",
    "            result.append(\"Box-3\")\n",
    "    return result\n",
    "\n",
    "'''\n",
    "When t = 2, for example, the chance of transitioning from Box 1(1) to Box 1(2) is greater than that of\n",
    "transitioning from Box-1(1) to Box-3(2), so we keep track of Box-1(1) - Box-1(2) and forget track of Box-1(1) - Box-3(2).\n",
    "Similarly, we do the same thing with each hidden state.\n",
    "\n",
    "Finally we will obtain the most probable path, whcih will be 'Box-1 -> Box-1 -> Box-3'\n",
    "'''\n",
    "print(\"The most likely sequence of state is:\", viterbi(visible_state, transition_probability, emission_probability, start_probability))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transition_probability': array([[0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.33333333]]), 'emission_probability': array([[0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.33333333]])}\n"
     ]
    }
   ],
   "source": [
    "########## Bonus  ######### \n",
    "\n",
    "### implementation of Baum-Welch \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### Assumption: 1. Unifrom distribution of initial probabilities \n",
    "              # 2. Uniform distribution of probabilites from hidden to visible state\n",
    "'''\n",
    "Input Arguments:\n",
    "        \n",
    "        start_probability: Initial state distribution  of dimension 3\n",
    "        visible_State: Observation sequence of length 3\n",
    "        General information on the expected transition and emission probabilities\n",
    "\n",
    "Returns:\n",
    "        transition_probability: State transition probability matrix of dimension 3 x 3\n",
    "        emission_probability: Emission probability of dimension 3*3\n",
    "\n",
    "'''\n",
    "hidden_states={'Box-1', 'Box-2', 'Box-3'}\n",
    "## Assign observation states to numeric numbers {a=0; b=1; e=2} for simplicicty\n",
    "visible_state = np.array((0,1,2)) # 0 represents {a}; 1 represents {b}; 2 represents {e} \n",
    "\n",
    "# Transition Probabilities\n",
    "transition_probability = np.ones((3, 3))\n",
    "transition_probability = transition_probability / np.sum(transition_probability, axis=1)\n",
    " \n",
    "# Emission Probabilities\n",
    "emission_probability = np.array(((1, 1, 1), (1, 1, 1), (1, 1, 1))) # unifrom distribution of probabibilities between \n",
    "                                                                   # hidden state and observation state\n",
    "emission_probability = emission_probability / np.sum(emission_probability, axis=1).reshape((-1, 1))\n",
    " \n",
    "# Equal Probabilities for the starting distribution\n",
    "start_probability = np.array((0.333, 0.333, 0.333))\n",
    "\n",
    "## Forward and Backward Probabilities can be used to express the numerator of the equation.\n",
    "def forward(visible_state, transition_probability, emission_probability, start_probability):\n",
    "    forward_seq = np.zeros((visible_state.shape[0], transition_probability.shape[0]))\n",
    "    forward_seq[0, :] = start_probability * emission_probability[:, visible_state[0]]\n",
    " \n",
    "    for t in range(1, visible_state.shape[0]):\n",
    "        for j in range(transition_probability.shape[0]):\n",
    "            forward_seq[t, j] = forward_seq[t - 1].dot(transition_probability[:, j]) * emission_probability[j, visible_state[t]]\n",
    " \n",
    "    return forward_seq\n",
    " \n",
    " \n",
    "def backward(visible_state, transition_probability, emission_probability):\n",
    "    backward_seq = np.zeros((visible_state.shape[0], transition_probability.shape[0]))\n",
    " \n",
    "    # setting backward_seq(T) = 1\n",
    "    backward_seq[visible_state.shape[0] - 1] = np.ones((transition_probability.shape[0]))\n",
    " \n",
    "    # Loop in backward way from T-1 \n",
    "    # To get the total joint probability for all transitions from hidden state i to hidden state j,\n",
    "    # ...we must add all T together. This would be the equation's numerator.\n",
    "    for t in range(visible_state.shape[0] - 2, -1, -1): # Due to python indexing the actual loop will be T-2 to 0\n",
    "        for j in range(transition_probability.shape[0]):\n",
    "            backward_seq[t, j] = (backward_seq[t + 1] * emission_probability[:, visible_state[t + 1]]).dot(transition_probability[j, :])\n",
    " \n",
    "    return backward_seq\n",
    " \n",
    "'''\n",
    "The Baum-Welch Algorithm is a variant of the Expectation Maximization (EM) algorithm \n",
    "which is used to determine the transition and emission Matrix. \n",
    "\n",
    "Steps:\n",
    "- Begin with initial transition and emission probability estimates. \n",
    "  We can either set equal probabilities at the start or define them at random.\n",
    "- Calculate the expected frequency of each transition/emission.\n",
    "- Based on those predictions, recalculate the probabilities of translation and emisssion (from latent variable).\n",
    "- Repeat until convergence\n",
    "''' \n",
    "def baum_welch(visible_state, transition_probability, emission_probability, start_probability, n_iter=100):\n",
    "    VS = transition_probability.shape[0]\n",
    "    T = len(visible_state)\n",
    " \n",
    "    for n in range(n_iter):\n",
    "        forward_seq = forward(visible_state, transition_probability, emission_probability, start_probability)\n",
    "        backward_seq = backward(visible_state, transition_probability, emission_probability)\n",
    " \n",
    "        xi = np.zeros((VS, VS, T - 1))\n",
    "        for t in range(T - 1):\n",
    "            denominator = np.dot(np.dot(forward_seq[t, :].T, transition_probability) * emission_probability[:, visible_state[t + 1]].T, backward_seq[t + 1, :])\n",
    "            for i in range(VS):\n",
    "                numerator = forward_seq [t, i] * transition_probability[i, :] * emission_probability[:, visible_state[t + 1]].T * backward_seq[t + 1, :].T\n",
    "                xi[i, :, t] = numerator / denominator\n",
    " \n",
    "        gamma = np.sum(xi, axis=1)\n",
    "        transition_probability = np.sum(xi, 2) / np.sum(gamma, axis=1).reshape((-1, 1))\n",
    " \n",
    "        # Add additional T'th element in gamma\n",
    "        gamma = np.hstack((gamma, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1))))\n",
    " \n",
    "        EP = emission_probability.shape[1]\n",
    "        denominator = np.sum(gamma, axis=1)\n",
    "        for l in range(EP):\n",
    "            emission_probability[:, l] = np.sum(gamma[:, visible_state == l], axis=1)\n",
    " \n",
    "        emission_probability = np.divide(emission_probability, denominator.reshape((-1, 1)))\n",
    " \n",
    "    return {\"transition_probability\":transition_probability, \"emission_probability\":emission_probability}\n",
    "''' \n",
    "The transition and emission probability results obtained using this approach are identical to the values\n",
    "presented in HMM modeling. \n",
    "'''\n",
    "'''\n",
    "Result: \n",
    "For a uniform distribution on initial probabailities and unifrom distrubtion from hidden state to visible state\n",
    "{'transition_probability': array([[0.33333333, 0.33333333, 0.33333333],\n",
    "       [0.33333333, 0.33333333, 0.33333333],\n",
    "       [0.33333333, 0.33333333, 0.33333333]]), \n",
    "       'emission_probability': array([[0.33333333, 0.33333333, 0.33333333],\n",
    "       [0.33333333, 0.33333333, 0.33333333],\n",
    "       [0.33333333, 0.33333333, 0.33333333]])}\n",
    "\n",
    "'''\n",
    " \n",
    "print(baum_welch(visible_state, transition_probability, emission_probability, start_probability, n_iter=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
